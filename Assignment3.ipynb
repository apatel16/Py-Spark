{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.26.22.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=App>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeQuotes(s):\n",
    "    \"\"\" Remove quotation marks from an input string \n",
    "    Args:\n",
    "        s (str): input string that might have the quote \"\" characters \n",
    "    Returns:\n",
    "        str: a string without the quote characters \n",
    "    \"\"\"\n",
    "    return \"\".join(i for i in s if i != '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parseDataFileLine(dataFileLine):\n",
    "    \"\"\" Parse a line of the data file using the specified regular expression pattern \n",
    "    Args:\n",
    "        datafileLine (str): input string that is a line from the data file \n",
    "    Returns:\n",
    "        str: a string parsed using the given regular expression and without the quote characters \n",
    "    \"\"\"\n",
    "    match = re.search(DATAFILE_PATTERN, dataFileLine)\n",
    "    if match is None:\n",
    "        print('Invalid datafile line: %s' % dataFileLine)\n",
    "        return (dataFileLine, -1) \n",
    "    elif match.group(1) == '\"id\"':\n",
    "        print('Header datafile line: %s' % dataFileLine)\n",
    "        return (dataFileLine, 0)\n",
    "    else:\n",
    "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4)) \n",
    "        return ((removeQuotes(match.group(1)), product), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GOOGLE_PATH = './Assignment3DataSet/Google.csv'\n",
    "GOOGLE_SMALL_PATH = './Assignment3DataSet/Google_small.csv'\n",
    "AMAZON_PATH = './Assignment3DataSet/Amazon.csv'\n",
    "AMAZON_SMALL_PATH = './Assignment3DataSet/Amazon_small.csv'\n",
    "GOLD_STANDARD_PATH = './Assignment3DataSet/Amazon_Google_perfectMapping.csv'\n",
    "STOPWORDS_PATH = './Assignment3DataSet/stopwords.txt'\n",
    "\n",
    "def parseData(filename):\n",
    "    \"\"\" Parse a data file \n",
    "    Args:\n",
    "        filename (str): input file name of the data file \n",
    "    Returns:\n",
    "        RDD: a RDD of parsed lines\n",
    "    \"\"\"\n",
    "    return (sc.textFile(filename, 4, 1).map(parseDataFileLine))\n",
    "\n",
    "def loadData(filename):\n",
    "    \"\"\" Load a data file \n",
    "    Args:\n",
    "        path (str): input file name of the data file \n",
    "    Returns:\n",
    "        RDD: a RDD of parsed valid lines \n",
    "    \"\"\"\n",
    "    raw = parseData(filename).cache()\n",
    "    failed = (raw.filter(lambda s: s[1] == -1).map(lambda s: s[0]))\n",
    "    \n",
    "    for line in failed.take(10):\n",
    "        print('%s - Invalid datafile line: %s' % (filename, line))\n",
    "    valid = (raw.filter(lambda s: s[1] == 1).map(lambda s: s[0]).cache())\n",
    "    print('%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' \n",
    "          % (filename, raw.count(),valid.count(), failed.count()))\n",
    "    \n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == (valid.count() + 1) \n",
    "   \n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Assignment3DataSet/Google_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\n",
      "./Assignment3DataSet/Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines\n",
      "./Assignment3DataSet/Amazon_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\n",
      "./Assignment3DataSet/Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
    "google = loadData(GOOGLE_PATH)\n",
    "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
    "amazon = loadData(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google: http://www.google.com/base/feeds/snippets/11448761432933644608: spanish vocabulary builder \"expand your vocabulary! contains fun lessons that both teach and entertain you'll quickly find yourself mastering new terms. includes games and more!\" \n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/8175198959985911471: topics presents: museums of world \"5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...\" \n",
      "\n",
      "google: http://www.google.com/base/feeds/snippets/18445827127704822533: sierrahome hse hallmark card studio special edition win 98 me 2000 xp \"hallmark card studio special edition (win 98 me 2000 xp)\" \"sierrahome\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in googleSmall.take(3):\n",
    "    print('google: %s: %s\\n' % (line[0], line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon: b000jz4hqo: clickart 950 000 - premier image pack (dvd-rom)  \"broderbund\"\n",
      "\n",
      "amazon: b0006zf55o: ca international - arcserve lap/desktop oem 30pk \"oem arcserve backup v11.1 win 30u for laptops and desktops\" \"computer associates\"\n",
      "\n",
      "amazon: b00004tkvy: noah's ark activity center (jewel case ages 3-8)  \"victory multimedia\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in amazonSmall.take(3):\n",
    "    print('amazon: %s: %s\\n' % (line[0], line[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ER as Text Similarity - Bags of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Tokenizing a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization \n",
    "    Args:\n",
    "        string (str): input string \n",
    "    Returns:\n",
    "        list: a list of tokens \n",
    "    \"\"\"\n",
    "    tokenList = re.split(split_regex, string.lower())\n",
    "    tokenList = [token for token in tokenList if token != '']\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "[]\n",
      "['123a', '456_b', '789c', '123a']\n",
      "['fox', 'fox']\n"
     ]
    }
   ],
   "source": [
    "print(simpleTokenize(quickbrownfox))\n",
    "print(simpleTokenize(' '))\n",
    "print(simpleTokenize('!!!!123A/456_B/789C.123A'))\n",
    "print(simpleTokenize('fox fox'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopfile = STOPWORDS_PATH\n",
    "stopwords = set(sc.textFile(stopfile).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the stopwords: {'who', 'i', 'off', 'at', 's', 'further', 'over', 'him', 'for', 'am', 'on', 'until', 'of', 'just', 'own', 'myself', 'into', 'theirs', 'no', 'having', 'hers', 'to', 'while', 'been', 'our', 'yourselves', 'an', 'why', 'so', 'only', 'about', 'have', 'most', 'then', 'yourself', 'those', 'whom', 'which', 'be', 'she', 'will', 'from', 'both', 'all', 'his', 'each', 'same', 'because', 'has', 'doing', 'them', 'out', 'their', 'my', 'the', 'not', 't', 'themselves', 'as', 'under', 'very', 'or', 'this', 'such', 'than', 'between', 'does', 'any', 'herself', 'were', 'up', 'during', 'that', 'and', 'nor', 'other', 'did', 'but', 'itself', 'if', 'had', 'me', 'how', 'where', 'was', 'these', 'being', 'do', 'yours', 'ourselves', 'above', 'too', 'should', 'ours', 'few', 'after', 'he', 'some', 'with', 'what', 'can', 'now', 'you', 'are', 'your', 'is', 'in', 'her', 'don', 'its', 'it', 'below', 'here', 'once', 'through', 'down', 'when', 'again', 'by', 'before', 'against', 'himself', 'we', 'they', 'a', 'more', 'there'}\n"
     ]
    }
   ],
   "source": [
    "print('These are the stopwords: %s' % stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords \n",
    "    Args:\n",
    "        string (str): input string \n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords \n",
    "    \"\"\"\n",
    "    tokenList = re.split(split_regex, string.lower())\n",
    "    tokenList = [token for token in tokenList if token != '' and token not in stopwords]\n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "[]\n",
      "['the_']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(quickbrownfox))\n",
    "print(tokenize(\"Why a the?\"))\n",
    "print(tokenize(\"Being at the_?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Tokenizing the small Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amazonRecToToken = amazonSmall.map(lambda rec: (rec[0], tokenize(rec[1])))\n",
    "googleRecToToken = googleSmall.map(lambda rec: (rec[0], tokenize(rec[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def countTokens(vendorRDD):\n",
    "    return vendorRDD.flatMap(lambda rec: rec[1]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22520 tokens in the combined datasets\n"
     ]
    }
   ],
   "source": [
    "totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken) \n",
    "print('There are %s tokens in the combined datasets' % totalTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22520 tokens in the combined datasets\n"
     ]
    }
   ],
   "source": [
    "total = amazonSmall.flatMap(lambda rec: tokenize(rec[1])).collect() + googleSmall.flatMap(lambda rec: tokenize(rec[1])).collect()\n",
    "print('There are %s tokens in the combined datasets' % totalTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d Amazon record with most tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findBiggestRecord(vendorRDD):\n",
    "    return vendorRDD.map(lambda rec: (rec[0], len(rec[1]))).takeOrdered(1, lambda rec: -1 * rec[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon record with ID \"b000o24l3q\" has the most tokens (1547)\n"
     ]
    }
   ],
   "source": [
    "biggestRecordAmazon = findBiggestRecord(amazonRecToToken)\n",
    "print('The Amazon record with ID \"%s\" has the most tokens (%s)' % (biggestRecordAmazon[0][0],\n",
    "biggestRecordAmazon[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: ER as Text Similarity - Weighted Bag-of-Words using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Implement a TF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tf(tokens):\n",
    "    \n",
    "    termFrequencyDict = {}\n",
    "    lengthOfTokensList = len(tokens)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in termFrequencyDict:\n",
    "            termFrequencyDict[token] += (1.0/lengthOfTokensList)\n",
    "        else:\n",
    "            termFrequencyDict[token] = (1.0/lengthOfTokensList)\n",
    "    \n",
    "    return termFrequencyDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 0.16666666666666666, 'brown': 0.16666666666666666, 'fox': 0.16666666666666666, 'jumps': 0.16666666666666666, 'lazy': 0.16666666666666666, 'dog': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "print(tf(tokenize(quickbrownfox)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one_': 0.6666666666666666, 'two': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(tf(tokenize('one_ one_ two!')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) Create a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpusRDD = googleRecToToken.union(amazonRecToToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Implement an IDFs Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF \n",
    "    Args:\n",
    "        corpus (RDD): input corpus \n",
    "    Returns:\n",
    "        RDD: a RDD of (token, IDF value) \n",
    "    \"\"\"\n",
    "    N = corpus.count()\n",
    "    uniqueTokens = corpus.map(lambda rec: set(rec[1]))\n",
    "    tokenCountPairTuple = uniqueTokens.flatMap(lambda rec: [(r, 1) for r in rec])\n",
    "    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda a, b: a+b)\n",
    "    \n",
    "    return tokenSumPairTuple.map(lambda rec: (rec[0], N/rec[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4772 unique tokens in the small datasets.\n"
     ]
    }
   ],
   "source": [
    "idfsSmall = idfs(amazonRecToToken.union(googleRecToToken)) \n",
    "uniqueTokenCount = idfsSmall.count()\n",
    "print('There are %s unique tokens in the small datasets.' % uniqueTokenCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software\n"
     ]
    }
   ],
   "source": [
    "tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]\n",
    "print(tokenSmallestIdf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2d) Tokens with the Smallest IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('software', 4.25531914893617), ('features', 6.896551724137931), ('new', 6.896551724137931), ('use', 7.017543859649122), ('complete', 7.2727272727272725), ('easy', 7.6923076923076925), ('cd', 8.333333333333334), ('create', 8.333333333333334), ('system', 8.333333333333334), ('windows', 8.51063829787234), ('1', 8.51063829787234)]\n"
     ]
    }
   ],
   "source": [
    "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
    "print(smallIDFTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2e) IDF Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.hist(small_idf_values, 50, log = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[token: string, value: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idfsToCountRow = idfsSmall.map(lambda x: Row(token=x[0], value=x[1]))\n",
    "idfsToCountDF = sqlContext.createDataFrame(idfsToCountRow)\n",
    "display(idfsToCountDF)\n",
    "pd = idfsToCountDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>950</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rom</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dvd</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v11</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca</td>\n",
       "      <td>57.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token       value\n",
       "0   950  400.000000\n",
       "1   rom   16.666667\n",
       "2   dvd   16.000000\n",
       "3   v11  400.000000\n",
       "4    ca   57.142857"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2f) Implement a TF-IDF Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf(tokens, idfs): \n",
    "    \"\"\" Compute TF-IDF \n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize \n",
    "        idfs (dictionary): record to IDF value \n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values \n",
    "    \"\"\"\n",
    "    tfs = tf(tokens)\n",
    "    tfIdfDict = {(token , tfs[token]*idfs[token]) for token in set(tokens)}\n",
    "    return dict(tfIdfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon record \"b000hkgj8k\" has tokens and weights:\n",
      "{'customizing': 16.666666666666664, 'psg': 33.33333333333333, 'autocad': 33.33333333333333, '2007': 3.5087719298245617, 'courseware': 66.66666666666666, 'autodesk': 8.333333333333332, 'interface': 3.0303030303030303}\n"
     ]
    }
   ],
   "source": [
    "recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1] \n",
    "idfsSmallWeights = idfsSmall.collectAsMap()\n",
    "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
    "print('Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: ER as Text Similarity - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Implement the Components of a cosineSimilarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value \n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the two input dictionaries \n",
    "    \"\"\"\n",
    "    return sum(a[tok] * b[tok] for tok in a.keys() if tok in b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product \n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value \n",
    "    Returns:\n",
    "        norm (float): the square root of the dot product value \n",
    "    \"\"\"\n",
    "    return math.sqrt(dotprod(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cossim(a, b):\n",
    "    \"\"\" Compute cosine similarity Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value \n",
    "    Returns:\n",
    "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and then by the norm of the second dictionary\n",
    "    \"\"\"\n",
    "    return (dotprod(a,b) / norm(a)) / norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 6.164414002968976\n",
      "0.8262970212292282\n"
     ]
    }
   ],
   "source": [
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 } \n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print(dp, nm)\n",
    "print(cossim(testVec1, testVec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Implement a cosineSimilarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary): \n",
    "    \"\"\" Compute cosine similarity between two strings \n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "    idfsDictionary (dictionary): a dictionary of IDF values\n",
    "    Returns:\n",
    "        cossim: cosine similarity value\n",
    "    \"\"\"\n",
    "    w1 = tfidf(tokenize(string1), idfsDictionary) \n",
    "    w2 = tfidf(tokenize(string2), idfsDictionary)\n",
    "    return cossim(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cossimAdobe = cosineSimilarity('Adobe Photoshop', 'Adobe Illustrator',idfsSmallWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057724338216303385"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossimAdobe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Perform Entity Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crossSmall = googleSmall.cartesian(amazonSmall).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def computeSimilarity(record):\n",
    "    \"\"\" Compute similarity on a combination record \n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record) \n",
    "    Returns:\n",
    "        3-tuple: (google URL, amazon ID, cosine similarity value) \n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallWeights) \n",
    "    return (googleURL, amazonID, cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities = (crossSmall.map(computeSimilarity).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def similar(amazonID, googleURL): \n",
    "    \"\"\" Return similarity value\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL \n",
    "    Returns:\n",
    "        similar: cosine similarity value \n",
    "    \"\"\"\n",
    "    return similarities.filter(lambda record: (record[0] == googleURL and record[1] == amazonID)).collect()[0][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.0003031719404513201.\n"
     ]
    }
   ],
   "source": [
    "similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is %s.' % similarityAmazonGoogle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Perform Entity Resolution with Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested similarity is 0.0003031719404513201.\n",
      "4772\n"
     ]
    }
   ],
   "source": [
    "def computeSimilarityBroadcast(record):\n",
    "    \"\"\" Compute similarity on a combination record, using Broadcast variable \n",
    "    Args:\n",
    "        record: a pair, (google record, amazon record) \n",
    "    Returns:\n",
    "        pair: a pair, (google URL, amazon ID, cosine similarity value) \n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = googleRec[0]\n",
    "    amazonID = amazonRec[0]\n",
    "    googleValue = googleRec[1]\n",
    "    amazonValue = amazonRec[1]\n",
    "    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallBroadcast.value)\n",
    "    return (googleURL, amazonID, cs)\n",
    " \n",
    "weights = idfsSmallWeights.copy()\n",
    "idfsSmallBroadcast = sc.broadcast(weights) \n",
    "similaritiesBroadcast = (crossSmall.map(computeSimilarityBroadcast).cache())\n",
    "    \n",
    "    \n",
    "def similarBroadcast(amazonID, googleURL):\n",
    "    \"\"\" Return similarity value, computed using Broadcast variable \n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL Returns:\n",
    "    similar: cosine similarity value \n",
    "    \"\"\"\n",
    "    return (similaritiesBroadcast.filter(lambda record: (record[0] == googleURL and record[1] == amazonID)) .collect()[0][2])\n",
    "\n",
    "similarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print('Requested similarity is %s.' % similarityAmazonGoogleBroadcast)\n",
    "print(len(idfsSmallBroadcast.value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Perform a Gold Standard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GOLDFILE_PATTERN = '^(.+),(.+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_goldfile_line(goldfile_line):\n",
    "    \"\"\" Parse a line from the 'golden standard' data file\n",
    "    Args:\n",
    "        goldfile_line: a line of data\n",
    "    Returns:\n",
    "        pair: ((key, 'gold', 1 if successful or else 0))\n",
    "    \"\"\"\n",
    "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
    "    if match is None:\n",
    "        print('Invalid goldfile line: %s' % goldfile_line)\n",
    "        return (goldfile_line, -1)\n",
    "    elif match.group(1) == '\"idAmazon\"':\n",
    "        print('Header datafile line: %s' % goldfile_line)\n",
    "        return (goldfile_line, 0)\n",
    "    else:\n",
    "        key = '%s %s' % (removeQuotes(match.group(1)), removeQuotes(match.group(2))) \n",
    "    return ((key, 'gold'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1301 lines, successfully parsed 1300 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "goldfile = GOLD_STANDARD_PATH \n",
    "gsRaw = (sc.textFile(goldfile).map(parse_goldfile_line).cache())\n",
    "gsFailed = (gsRaw.filter(lambda s: s[1] == -1).map(lambda s: s[0]))\n",
    "for line in gsFailed.take(10):\n",
    "    print('Invalid goldfile line: %s' % line)\n",
    "goldStandard = (gsRaw .filter(lambda s: s[1] == 1).map(lambda s: s[0]) .cache())\n",
    "print('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (gsRaw.count(), goldStandard.count(), \n",
    "                                                                                     gsFailed.count()))\n",
    "\n",
    "assert (gsFailed.count() == 0)\n",
    "assert (gsRaw.count() == (goldStandard.count() + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the “gold standard” data we can answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sims = similaritiesBroadcast.map(lambda rec: (rec[1]+\" \"+rec[0], rec[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b000jz4hqo http://www.google.com/base/feeds/snippets/11448761432933644608',\n",
       "  0.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b000jz4hqo http://www.google.com/base/feeds/snippets/18441480711193821750',\n",
       "  'gold')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldStandard.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trueDupsRDD = sims.join(goldStandard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b00066dd5m http://www.google.com/base/feeds/snippets/949436197920820923',\n",
       "  (0.07847275689637273, 'gold')),\n",
       " ('b0002bqrq8 http://www.google.com/base/feeds/snippets/18386073325514862606',\n",
       "  (0.14431063919825257, 'gold'))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trueDupsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = trueDupsRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sumValue = trueDupsRDD.map(lambda rec: rec[1][0]).reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26433257343519145"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgSimsDup = float(sumValue / count)\n",
    "avgSimsDup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonDupsRDD = sims.leftOuterJoin(goldStandard).filter(lambda rec: rec[1][1] == None).map(lambda rec: (rec[0], rec[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b00021xhzw http://www.google.com/base/feeds/snippets/18445827127704822533',\n",
       "  0.0),\n",
       " ('b000gzwjgc http://www.google.com/base/feeds/snippets/8175198959985911471',\n",
       "  0.0),\n",
       " ('b000aazr5i http://www.google.com/base/feeds/snippets/8175198959985911471',\n",
       "  0.0),\n",
       " ('b0009rgzgm http://www.google.com/base/feeds/snippets/8175198959985911471',\n",
       "  0.001603895822502505),\n",
       " ('b00006hvvo http://www.google.com/base/feeds/snippets/18445827127704822533',\n",
       "  0.00021888375577371882)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonDupsRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSimsNon = nonDupsRDD.map(lambda rec: rec[1]).sum()/float(nonDupsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 true duplicates.\n",
      "The average similarity of true duplicates is 0.26433257343519145.\n",
      "And for non duplicates, it is 0.0012347630465555242.\n"
     ]
    }
   ],
   "source": [
    "print('There are %s true duplicates.' % count)\n",
    "print('The average similarity of true duplicates is %s.' % avgSimsDup)\n",
    "print('And for non duplicates, it is %s.' % avgSimsNon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Tokenize the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon full dataset is 1363 products, Google full dataset is 3226 products\n"
     ]
    }
   ],
   "source": [
    "amazonFullRecToToken = amazon.map(lambda rec: (rec[0], tokenize(rec[1])))\n",
    "googleFullRecToToken = google.map(lambda rec: (rec[0], tokenize(rec[1])))\n",
    "print('Amazon full dataset is %s products, Google full dataset is %s products' % (amazonFullRecToToken.count(), googleFullRecToToken.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Compute IDFs and TF-IDFs for the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17078 unique tokens in the full datasets.\n"
     ]
    }
   ],
   "source": [
    "fullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)\n",
    "idfsFull = idfs(fullCorpusRDD)\n",
    "idfsFullCount = idfsFull.count()\n",
    "print('There are %s unique tokens in the full datasets.' % idfsFullCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfsFullWeights = idfsFull.collectAsMap()\n",
    "idfsFullBroadcast = sc.broadcast(idfsFullWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1363 Amazon weights and 3226 Google weights.\n"
     ]
    }
   ],
   "source": [
    "amazonWeightsRDD = amazonFullRecToToken.map(lambda x: (x[0], tfidf(x[1],idfsFullBroadcast.value)))\n",
    "googleWeightsRDD = googleFullRecToToken.map(lambda x: (x[0], tfidf(x[1],idfsFullBroadcast.value)))\n",
    "print('There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(), googleWeightsRDD.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Compute Norms for the weights from the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonNorms = amazonWeightsRDD.map(lambda x: (x[0], norm(x[1])))\n",
    "amazonNormsBroadcast = sc.broadcast(amazonNorms.collectAsMap())\n",
    "googleNorms = googleWeightsRDD.map(lambda x: (x[0], norm(x[1])))\n",
    "googleNormsBroadcast = sc.broadcast(googleNorms.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363\n",
      "3226\n"
     ]
    }
   ],
   "source": [
    "print(len(amazonNormsBroadcast.value))\n",
    "print(len(googleNormsBroadcast.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Create inverted indicies from the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('foo', 1)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def invert(record):\n",
    "    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n",
    "    Args:\n",
    "        record: a pair, (ID, token vector)\n",
    "    Returns:\n",
    "        pairs: a list of pairs of token to ID\n",
    "    \"\"\"\n",
    "    pairs = [(token, record[0]) for token in record[1]]\n",
    "    return pairs\n",
    "\n",
    "print(invert((1, {'foo': 2})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 111387 Amazon inverted pairs and 77678 Google inverted pairs.\n"
     ]
    }
   ],
   "source": [
    "amazonInvPairsRDD = (amazonWeightsRDD.flatMap(invert).cache())\n",
    "googleInvPairsRDD = (googleWeightsRDD.flatMap(invert).cache())\n",
    "\n",
    "print('There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),\n",
    "                                                                            googleInvPairsRDD.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Identify common tokens from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(record):\n",
    "    \"\"\" Swap (token, (ID, URL)) to ((ID, URL), token)\n",
    "    Args:\n",
    "        record: a pair, (token, (ID, URL))\n",
    "    Returns:\n",
    "        pair: ((ID, URL), token)\n",
    "    \"\"\"\n",
    "    token = record[0]\n",
    "    keys = record[1]\n",
    "    return (keys, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-c57916261d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcommonTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamazonInvPairsRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogleInvPairsRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %d common tokens'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcommonTokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "commonTokens = (amazonInvPairsRDD.join(googleInvPairsRDD).map(swap).groupByKey().cache())\n",
    "print('Found %d common tokens' % commonTokens.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Identify common tokens from the full dataset (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())\n",
    "googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastCosineSimilarity(record):\n",
    "    \"\"\" Compute Cosine Similarity using Broadcast variables\n",
    "    Args:\n",
    "        record: ((ID, URL), token)\n",
    "    Returns:\n",
    "        pair: ((ID, URL), cosine similarity value)\n",
    "    \"\"\"\n",
    "    amazonRec = record[0][0]\n",
    "    googleRec = record[0][1]\n",
    "    tokens = record[1]\n",
    "    s = sum(amazonWeightsBroadcast.value[amazonRec][i] * googleWeightsBroadcast.value[googleRec][i] for i in tokens)\n",
    "    value = s/(amazonNormsBroadcast.value[amazonRec] * googleNormsBroadcast.value[googleRec])\n",
    "    key = (amazonRec, googleRec)\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similaritiesFullRDD = (commonTokens.map(fastCosineSimilarity).cache())\n",
    "print(similaritiesFullRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityTest = similaritiesFullRDD.filter(lambda x: x[0][0] == 'b00005lzly' and x[0][1] == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()\n",
    "print(similarityTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5a) Counting True Positives, False Positives, and False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsFullRDD = similaritiesFullRDD.map(lambda x: (\"%s %s\" % (x[0][0], x[0][1]), x[1]))\n",
    "assert (simsFullRDD.count() == 2441100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsFullValuesRDD = (simsFullRDD\n",
    "                     .map(lambda x: x[1])\n",
    "                     .cache())\n",
    "assert (simsFullValuesRDD.count() == 2441100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_value(record):\n",
    "    if (record[1][1] is None):\n",
    "        return 0\n",
    "    else:\n",
    "        return record[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueDupSimsRDD = (goldStandard.leftOuterJoin(simsFullRDD).map(gs_value).cache())\n",
    "print('There are %s true duplicates.' % trueDupSimsRDD.count())\n",
    "assert(trueDupSimsRDD.count() == 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5b) Precision, Recall, and F-measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    # Initialize the VectorAccumulator to 0\n",
    "    def zero(self, value):\n",
    "        return [0] * len(value)\n",
    "\n",
    "    # Add two VectorAccumulator variables\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "            val1[i] += val2[i]\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list with entry x set to value and all other entries set to 0\n",
    "def set_bit(x, value, length):\n",
    "    bits = []\n",
    "    for y in range(length):\n",
    "        if (x == y):\n",
    "            bits.append(value)\n",
    "        else:\n",
    "            bits.append(0)\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-bin counts of false positives for different threshold ranges\n",
    "BINS = 101\n",
    "nthresholds = 100\n",
    "def bin(similarity):\n",
    "    return int(similarity * nthresholds)\n",
    "\n",
    "# fpCounts[i] = number of entries (possible false positives) where bin(similarity) == i\n",
    "zeros = [0] * BINS\n",
    "fpCounts = sc.accumulator(zeros, VectorAccumulatorParam())\n",
    "\n",
    "def add_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, 1, BINS)\n",
    "\n",
    "simsFullValuesRDD.foreach(add_element)\n",
    "\n",
    "# Remove true positives from FP counts\n",
    "def sub_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, -1, BINS)\n",
    "\n",
    "trueDupSimsRDD.foreach(sub_element)\n",
    "\n",
    "def falsepos(threshold):\n",
    "    fpList = fpCounts.value\n",
    "    return sum([fpList[b] for b in range(0, BINS) if float(b) / nthresholds >= threshold])\n",
    "\n",
    "def falseneg(threshold):\n",
    "    return trueDupSimsRDD.filter(lambda x: x < threshold).count()\n",
    "\n",
    "def truepos(threshold):\n",
    "    return trueDupSimsRDD.count() - falsenegDict[threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falseposDict[threshold])\n",
    "\n",
    "def recall(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falsenegDict[threshold])\n",
    "\n",
    "def fmeasure(threshold):\n",
    "    r = recall(threshold)\n",
    "    p = precision(threshold)\n",
    "    return 2 * r * p / (r + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5c) Line Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n",
    "falseposDict = dict([(t, falsepos(t)) for t in thresholds])\n",
    "falsenegDict = dict([(t, falseneg(t)) for t in thresholds])\n",
    "trueposDict = dict([(t, truepos(t)) for t in thresholds])\n",
    "\n",
    "precisions = [precision(t) for t in thresholds]\n",
    "recalls = [recall(t) for t in thresholds]\n",
    "fmeasures = [fmeasure(t) for t in thresholds]\n",
    "\n",
    "print(precisions[0], fmeasures[0])\n",
    "assert (abs(precisions[0] - 0.000532546802671) < 0.0000001)\n",
    "assert (abs(fmeasures[0] - 0.00106452669505) < 0.0000001)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)\n",
    "plt.plot(thresholds, fmeasures)\n",
    "plt.legend(['Precision', 'Recall', 'F-measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = [(t, precision(t), recall(t),fmeasure(t)) for t in thresholds]\n",
    "graphRDD = sc.parallelize(graph)\n",
    "\n",
    "graphRow = graphRDD.map(lambda txyz: Row(threshold=txyz[0], precision=txyz[1], recall=txyz[2], fmeasure=txyz[3]))\n",
    "graphDF = sqlContext.createDataFrame(graphRow)\n",
    "graphDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5d) Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
